
/*********************************************************************************************************************
  
  File Name 	:   CSLAB_DEEP_LEARNING_PAIR_INTERCOMPANY_TRANSACTION_AUTOENCODER_KMEANS
  Purpose 	:   A Program in Python for Pair Intercompany Transaction Using Autoencoder & Kmeans Clustering in Deep Learning
  Author	:   Durga Prasad
  Reviewer 	:   Jothi Periasamy
  Date and Time	:   08/01/2019 10:47 hrs
  Version	:   1.0	
  Change History: 
_______________________________________________________________________________________________________________________

   Who				   When 			        Why
_______________________________________________________________________________________________________________________


    DP				08/01/2019		           Initital Release 

________________________________________________________________________________________________________________________


/***********************************************************************************************************************

## Program Description : Program in Python for Pair Intercompany Transaction Using Autoencoder & Kmeans Clustering in Deep Learning

## Python Development Environment & Runtime - Python, Anaconda

# coding=utf-8

# Step 0 - Read Data from .INI File for Hardcoded Values

import sys
#print('Arguments:', len(sys.argv))
#vAR_Fetched_Data_INI_File_Path = sys.argv[1]
#print(vAR_Fetched_Data_INI_File_Path)

import configparser
vAR_Config = configparser.ConfigParser(allow_no_value=True)

import os

vAR_INI_File_Path = os.environ.get('PYTHON_TUTORIAL')

import pandas as vAR_pd

import configparser

import pandas as vAR_pd

vAR_Config = configparser.ConfigParser(allow_no_value=True)

vAR_Config.read(vAR_INI_File_Path)

vAR_CSLAB_INI_File = vAR_Config['Data File Path']['vAR_CSLAB_PROGRAM_357_INI_FILE']  

vAR_Config.read(vAR_CSLAB_INI_File)

vAR_Data = vAR_Config.sections()

vAR_Config.sections()

vAR_Fetched_Data_Source = vAR_Config['Data Source']['DATA_SOURCE1']
#print(vAR_Fetched_Data_Source)

vAR_Fetched_Data_Source_Connection_String = vAR_Config['Data Source Connection String']['SAP_CONNECTION_STRING']
#print(vAR_Fetched_Data_Source_Connection_String)

vAR_Fetched_Data_Source_Path_Input_Data = vAR_Config['Data Source Path']['INPUT_DATA_PATH']
#print(vAR_Fetched_Data_Source_Path_Input_Data)

vAR_Fetched_Data_Source_Path_Train_Data = vAR_Config['Data Source Path']['TRAIN_DATA_PATH']
#print(vAR_Fetched_Data_Source_Path_Train_Data)

vAR_Fetched_Data_Source_Path_Test_Data = vAR_Config['Data Source Path']['Test_DATA_PATH']
#print(vAR_Fetched_Data_Source_Path_Test_Data)

vAR_Fetched_Data_Train_Features1 = vAR_Config['Train Features - Problem 1']['FEATURE1']
#print(vAR_Fetched_Data_Train_Features1)

vAR_Fetched_Data_Train_Features2 = vAR_Config['Train Features - Problem 1']['FEATURE2']
#print(vAR_Fetched_Data_Train_Features2)

vAR_Fetched_Data_Train_Features3 = vAR_Config['Train Features - Problem 1']['FEATURE3']
#print(vAR_Fetched_Data_Train_Features3)

vAR_Fetched_Data_Train_Features4 = vAR_Config['Train Features - Problem 1']['FEATURE4']
#print(vAR_Fetched_Data_Train_Features4)

vAR_Fetched_Data_Train_All_Features = vAR_Config['Train Features - Problem 1']['ALL_FEATURES_TRAIN']
#print(vAR_Fetched_Data_Train_All_Features)

vAR_Fetched_Data_Test_Features1 = vAR_Config['Train Features - Problem 1']['FEATURE1']
#print(vAR_Fetched_Data_Test_Features1)

vAR_Fetched_Data_Test_Features2 = vAR_Config['Train Features - Problem 1']['FEATURE2']
#print(vAR_Fetched_Data_Test_Features2)

vAR_Fetched_Data_Test_Features3 = vAR_Config['Train Features - Problem 1']['FEATURE3']
#print(vAR_Fetched_Data_Test_Features3)

vAR_Fetched_Data_Test_Features4 = vAR_Config['Train Features - Problem 1']['FEATURE4']
#print(vAR_Fetched_Data_Test_Features4)

vAR_Fetched_Data_Test_All_Features = vAR_Config['Test Features - Problem 1']['ALL_FEATURES_TEST']
#print(vAR_Fetched_Data_Test_All_Features)

vAR_Fetched_Data_Model_Path = vAR_Config['Model Ouput Path']['MODEL_OUTPUT_PATH']
#print(vAR_Fetched_Data_Model_Path)

vAR_Fetched_Data_Best_Fit_File_Example_1 = vAR_Config['Model Fitting']['BEST_FIT_FILE_EXAMPLE_1']
#print(vAR_Fetched_Data_Best_Fit_File_Example_1)

vAR_Fetched_Data_Best_Fit_File_Example_2 = vAR_Config['Model Fitting']['BEST_FIT_FILE_EXAMPLE_2']
#print(vAR_Fetched_Data_Best_Fit_File_Example_2)

vAR_Fetched_Data_Best_Fit_File_Example_3 = vAR_Config['Model Fitting']['BEST_FIT_FILE_EXAMPLE_3']
#print(vAR_Fetched_Data_Best_Fit_File_Example_3)

vAR_Fetched_Data_Best_Fit_Image_Example_1 = vAR_Config['Model Fitting']['BEST_FIT_IMAGE_EXAMPLE_1']
#print(vAR_Fetched_Data_Best_Fit_Image_Example_1)

vAR_Fetched_Data_Best_Fit_Image_Example_2 = vAR_Config['Model Fitting']['BEST_FIT_IMAGE_EXAMPLE_2']
#print(vAR_Fetched_Data_Best_Fit_Image_Example_2)

vAR_Fetched_Data_Best_Fit_Image_Example_3 = vAR_Config['Model Fitting']['BEST_FIT_IMAGE_EXAMPLE_3']
#print(vAR_Fetched_Data_Best_Fit_Image_Example_3)

vAR_Fetched_Data_Under_Fit_File_Example_1 = vAR_Config['Model Fitting']['UNDER_FIT_FILE_EXAMPLE_1']
#print(vAR_Fetched_Data_Under_Fit_File_Example_1)

vAR_Fetched_Data_Under_Fit_File_Example_2 = vAR_Config['Model Fitting']['UNDER_FIT_FILE_EXAMPLE_2']
#print(vAR_Fetched_Data_Under_Fit_File_Example_2)

vAR_Fetched_Data_Under_Fit_File_Example_3 = vAR_Config['Model Fitting']['UNDER_FIT_FILE_EXAMPLE_3']
#print(vAR_Fetched_Data_Under_Fit_File_Example_3)

vAR_Fetched_Data_Under_Fit_Image_Example_1 = vAR_Config['Model Fitting']['UNDER_FIT_IMAGE_EXAMPLE_1']
#print(vAR_Fetched_Data_Under_Fit_Image_Example_1)

vAR_Fetched_Data_Under_Fit_Image_Example_2 = vAR_Config['Model Fitting']['UNDER_FIT_IMAGE_EXAMPLE_2']
#print(vAR_Fetched_Data_Under_Fit_Image_Example_2)

vAR_Fetched_Data_Under_Fit_Image_Example_3 = vAR_Config['Model Fitting']['UNDER_FIT_IMAGE_EXAMPLE_3']
#print(vAR_Fetched_Data_Under_Fit_Image_Example_3)

vAR_Fetched_Data_Over_Fit_File_Example_1 = vAR_Config['Model Fitting']['OVER_FIT_FILE_EXAMPLE_1']
#print(vAR_Fetched_Data_Over_Fit_File_Example_1)

vAR_Fetched_Data_Over_Fit_File_Example_2 = vAR_Config['Model Fitting']['OVER_FIT_FILE_EXAMPLE_2']
#print(vAR_Fetched_Data_Over_Fit_File_Example_2)

vAR_Fetched_Data_Over_Fit_File_Example_3 = vAR_Config['Model Fitting']['OVER_FIT_FILE_EXAMPLE_3']
#print(vAR_Fetched_Data_Over_Fit_File_Example_3)

vAR_Fetched_Data_Over_Fit_Image_Example_1 = vAR_Config['Model Fitting']['OvER_FIT_IMAGE_EXAMPLE_1']
#print(vAR_Fetched_Data_Over_Fit_Image_Example_1)

vAR_Fetched_Data_Over_Fit_Image_Example_2 = vAR_Config['Model Fitting']['OvER_FIT_IMAGE_EXAMPLE_2']
#print(vAR_Fetched_Data_Over_Fit_Image_Example_2)

vAR_Fetched_Data_Over_Fit_Image_Example_3 = vAR_Config['Model Fitting']['OVER_FIT_IMAGE_EXAMPLE_3']
#print(vAR_Fetched_Data_Over_Fit_Image_Example_3)

vAR_Fetched_Data_Cross_Validation_Image_Example_1 = vAR_Config['Model_Validation_Tuning']['CROSS_VALIDATION_IMAGE_EXAMPLE_1']
#print(vAR_Fetched_Data_Cross_Validation_Image_Example_1)

vAR_Fetched_Data_Cross_Validation_Image_Example_2 = vAR_Config['Model_Validation_Tuning']['CROSS_VALIDATION_IMAGE_EXAMPLE_2']
#print(vAR_Fetched_Data_Cross_Validation_Image_Example_2)

vAR_Fetched_Data_Cross_Validation_Image_Example_3 = vAR_Config['Model_Validation_Tuning']['CROSS_VALIDATION_IMAGE_EXAMPLE_3']
#print(vAR_Fetched_Data_Cross_Validation_Image_Example_3)

vAR_Fetched_Data_Before_Hyperparameter_Tuning_Image = vAR_Config['Model_Validation_Tuning']['BEFORE_HYPERPARAMETER_TUNING_IMAGE']
#print(vAR_Fetched_Data_Before_Hyperparameter_Tuning_Image)

vAR_Fetched_Data_After_Hyperparameter_Tuning_Image = vAR_Config['Model_Validation_Tuning']['AFTER_HYPERPARAMETER_TUNING_IMAGE']
#print(vAR_Fetched_Data_After_Hyperparameter_Tuning_Image)

vAR_Fetched_Data_Model_Fitting_Best_Fit_Test = vAR_Config['Model Fitting']['BEST_FIT_TEST_REQUIRED']
#print(vAR_Fetched_Data_Model_Fitting_Best_Fit_Test)

vAR_Fetched_Data_Model_Fitting_Under_Fit_Test = vAR_Config['Model Fitting']['UNDER_FIT_TEST_REQUIRED']
#print(vAR_Fetched_Data_Model_Fitting_Under_Fit_Test)

vAR_Fetched_Data_Model_Fitting_Over_Fit_Test = vAR_Config['Model Fitting']['OVER_FIT_TEST_REQUIRED']
#print(vAR_Fetched_Data_Model_Fitting_Over_Fit_Test)

vAR_Fetched_Data_Cross_Validation_Required = vAR_Config['Model_Validation_Tuning']['CROSS_VALIDATION_REQUIRED']
#print(Fetched_Data_Cross_Validation_Required)

vAR_Fetched_Data_Hyperparameter_Tuning_Required = vAR_Config['Model_Validation_Tuning']['HYPERPARAMETERS_TUNING_REQUIRED']
#print(Fetched_Data_Hyperparameter_Tuning_Required)

# Step 1 - Import the Required Libraries

#Our Model Implementation needs the Following Libraries:

#Sklearn: Sklearn is the Machine Learning Library which is used for numerical & scientific computations.

#Pandas: Pandas is a library used for data manipulation and analysis. 

#In Our Implementation. we are using it for Importing the Data file & Creating Dataframes (Stores the Data).

import pandas as vAR_pd
from sklearn.cluster import KMeans
from keras.layers import Input, Dense
from keras.models import Model
import numpy as np

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

#Step 2 - Import Clustering Data

#Next step after importing all libraries is getting the Training data imported. 

#We are importing the Clustering data stored in our local system with the use of Pandas library.

vAR_df = vAR_pd.read_excel(vAR_Fetched_Data_Source_Path_Train_Data)

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

# Step 3 – Convert Categorical Data into Numerical Values using Label Encoder 

# Next Step of the Implementation is Convertion of Categorical Data into Numerical Values & Feature Selection for Clustering.

vAR_le = LabelEncoder()
vAR_Transaction_Type_Conversion = vAR_le.fit_transform(vAR_df.iloc[:,7])
vAR_Transaction_Type_Conversion_df = vAR_pd.DataFrame(vAR_Transaction_Type_Conversion,columns={'Transaction_Type_Converted'})
vAR_Data_Category_Conversion = vAR_le.fit_transform(vAR_df.iloc[:,9])
vAR_Data_Category_Conversion_df = vAR_pd.DataFrame(vAR_Data_Category_Conversion,columns={'Data_Category_Converted'})
vAR_Doc_Date_Conversion = vAR_le.fit_transform(vAR_df.iloc[:,3])
vAR_Doc_Date_Conversion_df = vAR_pd.DataFrame(vAR_Doc_Date_Conversion,columns={'Doc_Date_Converted'})

# Attached the Converted Numerical Data to the main dataframe

vAR_df1 = vAR_df.merge(vAR_Transaction_Type_Conversion_df,left_index=True, right_index=True)
vAR_df2 = vAR_df1.merge(vAR_Data_Category_Conversion_df,left_index=True, right_index=True)
vAR_df3 = vAR_df2.merge(vAR_Doc_Date_Conversion_df,left_index=True, right_index=True)
vAR_df4 = vAR_pd.read_excel(vAR_Fetched_Data_Train_All_Features)
vAR_df4 = vAR_df4.values

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

# Step 4 – Clustering the Data

# Clustering the data meaning Making the model to group/cluster, understand & recognize the Pattern in the data..

vAR_df4 = vAR_df4.reshape(len(vAR_df4),np.prod(vAR_df4.shape[1:]))
#vAR_df4 = vAR_df4.reshape((vAR_df4.shape[0],vAR_df4.shape[1],1))

encoding_dim = 2 
input_dim = Input(shape=(2,))
encoded = Dense(encoding_dim, activation='relu')(input_dim)
decoded = Dense(2, activation='sigmoid')(encoded)
autoencoder = Model(input_dim, decoded)
autoencoder.compile(optimizer='adadelta', loss='categorical_crossentropy')

autoencoder.fit(vAR_df4,vAR_df4)
Pred = autoencoder.predict(vAR_df4)
Pred.astype(int)
vAR_df4 = vAR_pd.read_excel(vAR_Fetched_Data_Train_All_Features)

vAR_model = KMeans(n_clusters=12,random_state=0)
vAR_model.fit(vAR_df4)
vAR_Class = vAR_model.labels_
vAR_Class_df = vAR_pd.DataFrame(vAR_Class,columns={'New_Group'})
vAR_Class_df
vAR_df4_Class_Merge = vAR_df4.merge(vAR_Class_df,left_index=True, right_index=True) 
vAR_df4_Class_Merge
vAR_df5 = vAR_df4_Class_Merge.iloc[:,2]

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

# Step 5 – Review Learning Algorithm

# We Review the Algorithm as to see how it has learned from the Features we Provided

vAR_model.predict(vAR_df4)

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

# Step 6 - Import Test Data

# Importing the Test Data is to check how the data used on the Model Performs

vAR_df6 = vAR_pd.read_excel(vAR_Fetched_Data_Source_Path_Test_Data)
#vAR_Data_Category_Conversion_test = vAR_le.fit_transform(vAR_df5.iloc[:,9])
#vAR_Data_Category_Conversion_test_df = vAR_pd.DataFrame(vAR_Data_Category_Conversion_test,columns={'Data_Category_Converted_test'})
vAR_df7 = vAR_pd.read_excel(vAR_Fetched_Data_Test_All_Features)

vAR_Features_test = vAR_df7

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

# Step 7 – Running Model on Test Data

# Running the Model on Test Data is to use the imported test data to Prodict our Outcome

vAR_Labels_Pred = vAR_model.predict(vAR_Features_test)

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

# Step 8 – Review Model Outcome

# We check the Output of Model i.e the Prediction it has made on the test data

vAR_Labels_Pred = vAR_pd.DataFrame(vAR_Labels_Pred,columns={'Predicted_Inter_Transaction_Pair'})
#vAR_Features_test = vAR_Features_test.sort()

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

# Step 9 - Write Model Outcome to File

# Write the Model Output to an excel file for analysis.

vAR_df8 = vAR_pd.read_excel(vAR_Fetched_Data_Source_Path_Test_Data)
vAR_df9 = vAR_df8.merge(vAR_Labels_Pred,left_index=True, right_index=True)
vAR_df10 = vAR_df9.to_excel(vAR_Fetched_Data_Model_Path)

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

# Step 10 - To open and view the file outcome

# Open the Written File &amp; Check the Outcome as Shown. Execute to View the data

vAR_df11 = vAR_pd.read_excel(vAR_Fetched_Data_Model_Path)
vAR_df11

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#

/****************************************************************************************************************************
  Disclaimer.

We are providing this code block strictly for learning and researching, this is not a production
ready code. We have no liability on this particular code under any circumstances; users should use
this code on their own risk. All software, hardware and othr products that are referenced in these 
materials belong to the respective vendor who developed or who owns this product.

/****************************************************************************************************************************
  