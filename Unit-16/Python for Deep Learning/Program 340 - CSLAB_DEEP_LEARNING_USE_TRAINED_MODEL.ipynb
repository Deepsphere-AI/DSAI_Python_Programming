{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepsphere-AI/DSAI_Python_Programming/blob/main/Unit-16/Python%20for%20Deep%20Learning/Program%20340%20-%20CSLAB_DEEP_LEARNING_USE_TRAINED_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5LfbewjxNRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac69c475-afa2-4d4e-c768-f851a2c7f630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardcolab\n",
            "  Downloading tensorboardcolab-0.0.22.tar.gz (2.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tensorboardcolab\n",
            "  Building wheel for tensorboardcolab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorboardcolab: filename=tensorboardcolab-0.0.22-py3-none-any.whl size=3858 sha256=243467a6af8622095dc1bee4d489970dabca3d741132652adee76ce4d17f4b94\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/6b/92/99a181b543b45a45df4346bcdf01aac1f882fe447c63302878\n",
            "Successfully built tensorboardcolab\n",
            "Installing collected packages: tensorboardcolab\n",
            "Successfully installed tensorboardcolab-0.0.22\n",
            "Mounted at /content/drive\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 2s 205ms/step - loss: 0.6501 - accuracy: 0.7176 - val_loss: 1.0921 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.5869 - accuracy: 0.7176 - val_loss: 1.6549 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 0s 127ms/step - loss: 0.6028 - accuracy: 0.7176 - val_loss: 1.4082 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 272ms/step - loss: 0.6469 - accuracy: 0.7176 - val_loss: 1.4641 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 192ms/step - loss: 0.5870 - accuracy: 0.7176 - val_loss: 1.1808 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 1s 188ms/step - loss: 0.5842 - accuracy: 0.7176 - val_loss: 1.2140 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 2s 573ms/step - loss: 0.6081 - accuracy: 0.7176 - val_loss: 1.6882 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 2s 646ms/step - loss: 0.5742 - accuracy: 0.7176 - val_loss: 0.9281 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 3s 864ms/step - loss: 0.6368 - accuracy: 0.7176 - val_loss: 0.9677 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 2s 319ms/step - loss: 0.6933 - accuracy: 0.4941 - val_loss: 0.7945 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 170ms/step - loss: 0.6387 - accuracy: 0.7176 - val_loss: 1.0181 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 0s 167ms/step - loss: 0.6023 - accuracy: 0.7176 - val_loss: 1.4204 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 2s 314ms/step - loss: 0.6873 - accuracy: 0.7176 - val_loss: 0.7195 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 181ms/step - loss: 0.6805 - accuracy: 0.7176 - val_loss: 0.7358 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.6739 - accuracy: 0.7176 - val_loss: 0.7501 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 263ms/step - loss: 0.6441 - accuracy: 0.7176 - val_loss: 1.4650 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 194ms/step - loss: 0.6108 - accuracy: 0.7176 - val_loss: 1.1818 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 1s 180ms/step - loss: 0.5882 - accuracy: 0.7176 - val_loss: 1.4526 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 296ms/step - loss: 0.6947 - accuracy: 0.4706 - val_loss: 0.6983 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 205ms/step - loss: 0.6905 - accuracy: 0.7176 - val_loss: 0.7016 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 1s 280ms/step - loss: 0.6890 - accuracy: 0.7176 - val_loss: 0.7061 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 4s 1s/step - loss: 0.6174 - accuracy: 0.6000 - val_loss: 3.0866 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 493ms/step - loss: 0.7627 - accuracy: 0.7176 - val_loss: 1.0927 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 1s 480ms/step - loss: 0.6128 - accuracy: 0.7176 - val_loss: 0.8870 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 2s 562ms/step - loss: 0.6875 - accuracy: 0.7176 - val_loss: 0.7228 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 481ms/step - loss: 0.6785 - accuracy: 0.7176 - val_loss: 0.7462 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 1s 479ms/step - loss: 0.6692 - accuracy: 0.7176 - val_loss: 0.7733 - val_accuracy: 0.0000e+00\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.6961 - accuracy: 0.5000\n",
            "0.6961082220077515\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# /*********************************************************************************************************************\n",
        "  \n",
        "#   File Name \t:   CSLAB_DEEP_LEARNING_USE_TRAINED_MODEL\n",
        "#   Purpose \t:   A Program in Python for Using a Trained Model using Deep Learning\n",
        "#   Author\t:   Deepsphere.ai\n",
        "#   Reviewer \t:   Jothi Periasamy\n",
        "#   Date : 25/10/2022\n",
        "#   Version\t:   1.0\t\n",
        " \n",
        "# /***********************************************************************************************************************\n",
        "\n",
        "## Program Description : Program for Using a Trained Model using Deep Learning in Python\n",
        "\n",
        "## Python Development Environment & Runtime - Python, Anaconda\n",
        "\n",
        "import tensorflow as vAR_tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "%load_ext tensorboard\n",
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "vAR_CSLAB_pickle_in = open(\"/content/drive/MyDrive/Semester-4/Deep Learning for Enterprise/Model/vAR_CSLAB_X.pickle\",\"rb\")\n",
        "vAR_CSLAB_X = pickle.load(vAR_CSLAB_pickle_in)\n",
        "\n",
        "vAR_CSLAB_pickle_in = open(\"/content/drive/MyDrive/Semester-4/Deep Learning for Enterprise/Model/vAR_CSLAB_y.pickle\",\"rb\")\n",
        "vAR_CSLAB_y = pickle.load(vAR_CSLAB_pickle_in)\n",
        "\n",
        "vAR_CSLAB_X = vAR_CSLAB_X/255.0\n",
        "\n",
        "vAR_CSLAB_dense_layers = [0, 1, 2]\n",
        "\n",
        "vAR_CSLAB_layer_sizes = [32, 64, 128]\n",
        "\n",
        "vAR_CSLAB_conv_layers = [1, 2, 3]\n",
        "\n",
        "for dense_layer in vAR_CSLAB_dense_layers:\n",
        "    for layer_size in vAR_CSLAB_layer_sizes:\n",
        "        for conv_layer in vAR_CSLAB_conv_layers:\n",
        "            vAR_CSLAB_NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
        "            \n",
        "            vAR_CSLAB_model = Sequential()\n",
        "\n",
        "            vAR_CSLAB_model.add(Conv2D(layer_size, (3, 3), input_shape=vAR_CSLAB_X.shape[1:]))\n",
        "            vAR_CSLAB_model.add(Activation('relu'))\n",
        "            vAR_CSLAB_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "            for l in range(conv_layer-1): \n",
        "                \n",
        "                vAR_CSLAB_model.add(Conv2D(layer_size, (3, 3)))\n",
        "                vAR_CSLAB_model.add(Activation('relu'))\n",
        "                vAR_CSLAB_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        vAR_CSLAB_model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "        \n",
        "        for _ in range(dense_layer):\n",
        "            \n",
        "            vAR_CSLAB_model.add(Dense(layer_size))\n",
        "            vAR_CSLAB_model.add(Activation('relu'))\n",
        "\n",
        "            vAR_CSLAB_model.add(Dense(1))\n",
        "            vAR_CSLAB_model.add(Activation('sigmoid'))\n",
        "\n",
        "            #vAR_CSLAB_logdir = \"/content/drive/MyDrive/Semester-4/Deep Learning for Enterprise/\" + \"logs/fit2/\"\n",
        "            \n",
        "            #vAR_CSLAB_tensorboard_callback = vAR_tf.keras.callbacks.TensorBoard(vAR_CSLAB_logdir, histogram_freq=1)\n",
        "            \n",
        "            vAR_CSLAB_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "            vAR_CSLAB_model.fit(vAR_CSLAB_X, vAR_CSLAB_y,batch_size=32,epochs=3,validation_split=0.3)\n",
        "            \n",
        "vAR_CSLAB_model.save('/content/drive/MyDrive/Semester-4/Deep Learning for Enterprise/Model/64x3-CNN.model.h5')\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "vAR_loaded_model = load_model(\"/content/drive/MyDrive/Semester-4/Deep Learning for Enterprise/Model/64x3-CNN.model.h5\")\n",
        "\n",
        "vAR_CSLAB_loss, vAR_CSLAB_accuracy = vAR_loaded_model.evaluate(vAR_CSLAB_X, vAR_CSLAB_y)\n",
        "\n",
        "print(vAR_CSLAB_loss)\n",
        "\n",
        "# /****************************************************************************************************************************\n",
        "#   Disclaimer.\n",
        "\n",
        "# We are providing this code block strictly for learning and researching, this is not a production\n",
        "# ready code. We have no liability on this particular code under any circumstances; users should use\n",
        "# this code on their own risk. All software, hardware and othr products that are referenced in these \n",
        "# materials belong to the respective vendor who developed or who owns this product.\n",
        "\n",
        "# /****************************************************************************************************************************\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Explanation**\n",
        "\n",
        "This code is a script for training a Convolutional Neural Network (CNN) using the Keras API in Tensorflow.\n",
        "\n",
        "The script starts by importing the required libraries: Tensorflow, Keras, and pickle. Tensorboard is also imported to visualize the training process, and TensorboardColab is used to display the Tensorboard in Colab."
      ],
      "metadata": {
        "id": "Fq-mQHVAh-14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Google Drive is mounted to access the stored pickle files that contain the dataset, which is then loaded using the pickle library. The dataset is then normalized by dividing all values by 255.\n",
        "\n",
        "The script then defines three lists for the hyperparameters of the model: dense layers, layer sizes, and convolutional layers. These hyperparameters are then looped over to create different models with different combinations of hyperparameters."
      ],
      "metadata": {
        "id": "EIPXIojXiAnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each combination of hyperparameters, the model is defined using the Keras Sequential model. It starts by adding a Conv2D layer with a specified layer size and 3x3 filter size. It then adds an activation function 'relu' and a MaxPooling2D layer. The number of Conv2D and MaxPooling2D layers is determined by the 'conv_layer' variable.\n",
        "\n",
        "The model then flattens the feature maps and adds dense layers with the specified number of nodes, determined by the 'layer_size' variable. The activation function for the dense layers is 'relu' and the final layer is a single node with a sigmoid activation function to produce binary predictions."
      ],
      "metadata": {
        "id": "TWQCiQBFiAyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is then compiled using binary cross-entropy loss and the Adam optimizer. It is then fit to the training data with a batch size of 32 and 3 epochs. The validation split is set to 0.3, meaning 30% of the data will be used for validation and 70% for training.\n",
        "\n",
        "Finally, the trained model is saved to the Google Drive and reloaded using the load_model function. The loaded model is then evaluated on the complete dataset and the loss is printed."
      ],
      "metadata": {
        "id": "xM1tBOpHiA-l"
      }
    }
  ]
}