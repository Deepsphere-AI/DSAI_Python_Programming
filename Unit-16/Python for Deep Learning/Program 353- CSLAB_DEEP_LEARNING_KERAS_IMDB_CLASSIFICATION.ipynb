{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepsphere-AI/DSAI_Python_Programming/blob/main/Unit-16/Python%20for%20Deep%20Learning/Program%20353-%20CSLAB_DEEP_LEARNING_KERAS_IMDB_CLASSIFICATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDFgSK9Z8Hjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d090a0-3064-4e9e-91be-1d8cc987d470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "x_train shape: (25000, 400)\n",
            "x_test shape: (25000, 400)\n",
            "Build model...\n",
            "782/782 [==============================] - 121s 151ms/step - loss: 0.4116 - accuracy: 0.7975 - val_loss: 0.2848 - val_accuracy: 0.8788\n",
            "782/782 [==============================] - 23s 29ms/step - loss: 0.2848 - accuracy: 0.8788\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28475964069366455, 0.8788400292396545]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "# *********************************************************************************************************************\n",
        "  \n",
        "  # File Name \t:   CSLAB_DEEP_LEARNING_KERAS_IMDB_CLASSIFICATION\n",
        "  # Purpose \t:   A Program in Python for IMDB Movie Classification using Keras Library in Deep Learning\n",
        "  # Author\t:   Deepsphere.ai\n",
        "  # Reviewer \t:   Jothi Periasamy\n",
        "  # Date \t:   28/10/2022\n",
        "  # Version\t:   1.0\t\n",
        "  \n",
        "# ***********************************************************************************************************************\n",
        "\n",
        "## Program Description : Program in Python for IMDB Movie Classification using Keras Library in Deep Learning\n",
        "\n",
        "## Python Development Environment & Runtime - Python, Anaconda\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb#\n",
        "from keras.utils import pad_sequences\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# set parameters:\n",
        "vAR_CSLAB_max_features = 5000\n",
        "vAR_CSLAB_maxlen = 400\n",
        "vAR_CSLAB_batch_size = 32\n",
        "vAR_CSLAB_embedding_dims = 50\n",
        "vAR_CSLAB_filters = 250\n",
        "vAR_CSLAB_kernel_size = 3\n",
        "vAR_CSLAB_hidden_dims = 250\n",
        "vAR_CSLAB_epochs = 1\n",
        "\n",
        "print('Loading data...')\n",
        "(vAR_CSLAB_x_train, vAR_CSLAB_y_train), (vAR_CSLAB_x_test, vAR_CSLAB_y_test) = imdb.load_data(num_words=vAR_CSLAB_max_features)\n",
        "print(len(vAR_CSLAB_x_train), 'train sequences')\n",
        "print(len(vAR_CSLAB_x_test), 'test sequences')\n",
        "\n",
        "#print('Pad sequences (samples x time)')\n",
        "vAR_CSLAB_x_train = pad_sequences(vAR_CSLAB_x_train, maxlen=vAR_CSLAB_maxlen)\n",
        "vAR_CSLAB_x_test = pad_sequences(vAR_CSLAB_x_test, maxlen=vAR_CSLAB_maxlen)\n",
        "print('x_train shape:', vAR_CSLAB_x_train.shape)\n",
        "print('x_test shape:', vAR_CSLAB_x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "vAR_CSLAB_model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "vAR_CSLAB_model.add(Embedding(vAR_CSLAB_max_features,\n",
        "                    vAR_CSLAB_embedding_dims,\n",
        "                    input_length=vAR_CSLAB_maxlen))\n",
        "vAR_CSLAB_model.add(Dropout(0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "vAR_CSLAB_model.add(Conv1D(vAR_CSLAB_filters,\n",
        "                 vAR_CSLAB_kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "# we use max pooling:\n",
        "vAR_CSLAB_model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "vAR_CSLAB_model.add(Dense(vAR_CSLAB_hidden_dims))\n",
        "vAR_CSLAB_model.add(Dropout(0.2))\n",
        "vAR_CSLAB_model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "vAR_CSLAB_model.add(Dense(1))\n",
        "vAR_CSLAB_model.add(Activation('sigmoid'))\n",
        "\n",
        "vAR_CSLAB_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "vAR_CSLAB_model.fit(vAR_CSLAB_x_train, vAR_CSLAB_y_train,\n",
        "          batch_size=vAR_CSLAB_batch_size,\n",
        "          epochs=vAR_CSLAB_epochs,\n",
        "          validation_data=(vAR_CSLAB_x_test, vAR_CSLAB_y_test))\n",
        "\n",
        "vAR_CSLAB_model.evaluate(vAR_CSLAB_x_test,vAR_CSLAB_y_test)\n",
        "\n",
        "# ****************************************************************************************************************************\n",
        "#   Disclaimer.\n",
        "\n",
        "# We are providing this code block strictly for learning and researching, this is not a production\n",
        "# ready code. We have no liability on this particular code under any circumstances; users should use\n",
        "# this code on their own risk. All software, hardware and othr products that are referenced in these \n",
        "# materials belong to the respective vendor who developed or who owns this product.\n",
        "\n",
        "# ****************************************************************************************************************************\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Explanation**\n",
        "\n",
        "The code is a script that trains a binary sentiment classification model using the IMDB dataset, which consists of movie reviews that have been labeled as positive or negative. The model uses a Convolutional Neural Network (CNN) architecture. The script is written in Python and uses the Keras library for building and training the model."
      ],
      "metadata": {
        "id": "ZSiciFsuLWV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code starts with importing several modules from the Keras library, including Sequential, Dense, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D, and pad_sequences. The from __future__ import print_function statement is a way of importing print statements from future versions of Python into the current version of Python.\n",
        "\n",
        "The script then sets several hyperparameters that control the model's architecture and training process, such as the number of words to consider as features (vAR_CSLAB_max_features), the length of the input sequences (vAR_CSLAB_maxlen), the batch size (vAR_CSLAB_batch_size), the size of the word embeddings (vAR_CSLAB_embedding_dims), the number of filters in the Conv1D layer (vAR_CSLAB_filters), the size of the filters (vAR_CSLAB_kernel_size), the size of the hidden layer (vAR_CSLAB_hidden_dims), and the number of epochs to train the model (vAR_CSLAB_epochs)."
      ],
      "metadata": {
        "id": "rYZ3BuN-LWSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code then loads the IMDB dataset using the imdb.load_data function and pads the input sequences so that they all have the same length (vAR_CSLAB_maxlen).\n",
        "\n",
        "The model architecture is defined next using the Keras Sequential model. The model starts with an embedding layer that maps the input sequences (word indices) into continuous vectors (word embeddings) with vAR_CSLAB_embedding_dims dimensions. The embedding layer is followed by a dropout layer that randomly sets a fraction of the input values to zero to prevent overfitting.\n"
      ],
      "metadata": {
        "id": "XbA8NHA2LWHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next layer is a Conv1D layer that learns filters for word groupings in the input sequences. The Conv1D layer is followed by a global max pooling layer that takes the maximum value from each feature map generated by the Conv1D layer.\n",
        "\n",
        "The model then has a dense hidden layer with vAR_CSLAB_hidden_dims units and a dropout layer, followed by an activation layer with a ReLU activation function.\n"
      ],
      "metadata": {
        "id": "elzmc6f0LV_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final layer is a dense output layer with a single unit and a sigmoid activation function, which generates a binary output indicating the sentiment of the input movie review.\n",
        "\n",
        "The model is then compiled using the binary_crossentropy loss function, the Adam optimizer, and the accuracy metric. The model is trained using the training data (vAR_CSLAB_x_train and vAR_CSLAB_y_train) for vAR_CSLAB_epochs epochs with a batch size of vAR_CSLAB_batch_size.\n",
        "\n",
        "Finally, the model is evaluated on the test data (vAR_CSLAB_x_test and vAR_CSLAB_y_test) and the evaluation results are displayed."
      ],
      "metadata": {
        "id": "yjUizmabLV14"
      }
    }
  ]
}