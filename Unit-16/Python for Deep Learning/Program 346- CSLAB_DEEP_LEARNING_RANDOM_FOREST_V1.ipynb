{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepsphere-AI/DSAI_Python_Programming/blob/main/Unit-16/Python%20for%20Deep%20Learning/Program%20346-%20CSLAB_DEEP_LEARNING_RANDOM_FOREST_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reTxotRysKeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0597b1ec-bba7-4b1d-8262-e5d714a7d901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Train on 112 samples\n",
            "Epoch 1/100\n",
            "112/112 [==============================] - 0s 3ms/sample - loss: 1.5151 - acc: 0.2232\n",
            "Epoch 2/100\n",
            "112/112 [==============================] - 0s 394us/sample - loss: 1.4185 - acc: 0.3214\n",
            "Epoch 3/100\n",
            "112/112 [==============================] - 0s 428us/sample - loss: 1.3901 - acc: 0.3571\n",
            "Epoch 4/100\n",
            "112/112 [==============================] - 0s 347us/sample - loss: 1.3750 - acc: 0.3750\n",
            "Epoch 5/100\n",
            "112/112 [==============================] - 0s 359us/sample - loss: 1.2964 - acc: 0.4286\n",
            "Epoch 6/100\n",
            "112/112 [==============================] - 0s 430us/sample - loss: 1.2766 - acc: 0.4196\n",
            "Epoch 7/100\n",
            "112/112 [==============================] - 0s 401us/sample - loss: 1.2482 - acc: 0.4107\n",
            "Epoch 8/100\n",
            "112/112 [==============================] - 0s 608us/sample - loss: 1.2346 - acc: 0.4375\n",
            "Epoch 9/100\n",
            "112/112 [==============================] - 0s 368us/sample - loss: 1.1858 - acc: 0.5000\n",
            "Epoch 10/100\n",
            "112/112 [==============================] - 0s 411us/sample - loss: 1.2125 - acc: 0.5268\n",
            "Epoch 11/100\n",
            "112/112 [==============================] - 0s 383us/sample - loss: 1.1551 - acc: 0.5536\n",
            "Epoch 12/100\n",
            "112/112 [==============================] - 0s 444us/sample - loss: 1.1190 - acc: 0.5625\n",
            "Epoch 13/100\n",
            "112/112 [==============================] - 0s 353us/sample - loss: 1.0794 - acc: 0.5982\n",
            "Epoch 14/100\n",
            "112/112 [==============================] - 0s 361us/sample - loss: 1.0946 - acc: 0.5982\n",
            "Epoch 15/100\n",
            "112/112 [==============================] - 0s 464us/sample - loss: 1.0356 - acc: 0.6518\n",
            "Epoch 16/100\n",
            "112/112 [==============================] - 0s 397us/sample - loss: 0.9899 - acc: 0.6339\n",
            "Epoch 17/100\n",
            "112/112 [==============================] - 0s 325us/sample - loss: 1.0881 - acc: 0.6161\n",
            "Epoch 18/100\n",
            "112/112 [==============================] - 0s 356us/sample - loss: 1.0233 - acc: 0.6518\n",
            "Epoch 19/100\n",
            "112/112 [==============================] - 0s 370us/sample - loss: 0.9098 - acc: 0.7143\n",
            "Epoch 20/100\n",
            "112/112 [==============================] - 0s 374us/sample - loss: 1.0931 - acc: 0.6250\n",
            "Epoch 21/100\n",
            "112/112 [==============================] - 0s 342us/sample - loss: 0.9308 - acc: 0.6607\n",
            "Epoch 22/100\n",
            "112/112 [==============================] - 0s 412us/sample - loss: 0.8738 - acc: 0.7054\n",
            "Epoch 23/100\n",
            "112/112 [==============================] - 0s 401us/sample - loss: 0.9181 - acc: 0.7232\n",
            "Epoch 24/100\n",
            "112/112 [==============================] - 0s 337us/sample - loss: 0.9092 - acc: 0.7054\n",
            "Epoch 25/100\n",
            "112/112 [==============================] - 0s 351us/sample - loss: 0.9838 - acc: 0.6161\n",
            "Epoch 26/100\n",
            "112/112 [==============================] - 0s 370us/sample - loss: 0.8407 - acc: 0.7054\n",
            "Epoch 27/100\n",
            "112/112 [==============================] - 0s 347us/sample - loss: 0.8143 - acc: 0.7321\n",
            "Epoch 28/100\n",
            "112/112 [==============================] - 0s 331us/sample - loss: 0.8980 - acc: 0.6786\n",
            "Epoch 29/100\n",
            "112/112 [==============================] - 0s 356us/sample - loss: 0.8973 - acc: 0.6786\n",
            "Epoch 30/100\n",
            "112/112 [==============================] - 0s 338us/sample - loss: 0.8510 - acc: 0.6786\n",
            "Epoch 31/100\n",
            "112/112 [==============================] - 0s 373us/sample - loss: 0.8240 - acc: 0.7500\n",
            "Epoch 32/100\n",
            "112/112 [==============================] - 0s 465us/sample - loss: 0.9031 - acc: 0.6875\n",
            "Epoch 33/100\n",
            "112/112 [==============================] - 0s 391us/sample - loss: 0.8202 - acc: 0.7500\n",
            "Epoch 34/100\n",
            "112/112 [==============================] - 0s 417us/sample - loss: 0.7711 - acc: 0.7411\n",
            "Epoch 35/100\n",
            "112/112 [==============================] - 0s 457us/sample - loss: 0.7611 - acc: 0.7411\n",
            "Epoch 36/100\n",
            "112/112 [==============================] - 0s 540us/sample - loss: 0.7968 - acc: 0.7589\n",
            "Epoch 37/100\n",
            "112/112 [==============================] - 0s 392us/sample - loss: 0.7281 - acc: 0.8036\n",
            "Epoch 38/100\n",
            "112/112 [==============================] - 0s 409us/sample - loss: 0.7223 - acc: 0.7500\n",
            "Epoch 39/100\n",
            "112/112 [==============================] - 0s 500us/sample - loss: 0.7220 - acc: 0.7321\n",
            "Epoch 40/100\n",
            "112/112 [==============================] - 0s 349us/sample - loss: 0.6845 - acc: 0.7679\n",
            "Epoch 41/100\n",
            "112/112 [==============================] - 0s 338us/sample - loss: 0.6807 - acc: 0.7500\n",
            "Epoch 42/100\n",
            "112/112 [==============================] - 0s 333us/sample - loss: 0.7481 - acc: 0.7589\n",
            "Epoch 43/100\n",
            "112/112 [==============================] - 0s 454us/sample - loss: 0.7539 - acc: 0.7500\n",
            "Epoch 44/100\n",
            "112/112 [==============================] - 0s 419us/sample - loss: 0.8545 - acc: 0.6518\n",
            "Epoch 45/100\n",
            "112/112 [==============================] - 0s 341us/sample - loss: 0.7181 - acc: 0.8036\n",
            "Epoch 46/100\n",
            "112/112 [==============================] - 0s 331us/sample - loss: 0.7696 - acc: 0.7232\n",
            "Epoch 47/100\n",
            "112/112 [==============================] - 0s 354us/sample - loss: 0.6310 - acc: 0.8304\n",
            "Epoch 48/100\n",
            "112/112 [==============================] - 0s 366us/sample - loss: 0.8192 - acc: 0.7411\n",
            "Epoch 49/100\n",
            "112/112 [==============================] - 0s 377us/sample - loss: 0.7337 - acc: 0.7411\n",
            "Epoch 50/100\n",
            "112/112 [==============================] - 0s 343us/sample - loss: 0.6888 - acc: 0.7679\n",
            "Epoch 51/100\n",
            "112/112 [==============================] - 0s 406us/sample - loss: 0.8126 - acc: 0.7232\n",
            "Epoch 52/100\n",
            "112/112 [==============================] - 0s 416us/sample - loss: 0.6470 - acc: 0.7589\n",
            "Epoch 53/100\n",
            "112/112 [==============================] - 0s 417us/sample - loss: 0.6486 - acc: 0.7143\n",
            "Epoch 54/100\n",
            "112/112 [==============================] - 0s 345us/sample - loss: 0.7311 - acc: 0.7411\n",
            "Epoch 55/100\n",
            "112/112 [==============================] - 0s 467us/sample - loss: 0.5771 - acc: 0.8482\n",
            "Epoch 56/100\n",
            "112/112 [==============================] - 0s 337us/sample - loss: 0.6618 - acc: 0.7857\n",
            "Epoch 57/100\n",
            "112/112 [==============================] - 0s 485us/sample - loss: 0.5729 - acc: 0.8393\n",
            "Epoch 58/100\n",
            "112/112 [==============================] - 0s 377us/sample - loss: 0.8967 - acc: 0.6607\n",
            "Epoch 59/100\n",
            "112/112 [==============================] - 0s 441us/sample - loss: 0.6999 - acc: 0.7500\n",
            "Epoch 60/100\n",
            "112/112 [==============================] - 0s 447us/sample - loss: 0.6806 - acc: 0.7768\n",
            "Epoch 61/100\n",
            "112/112 [==============================] - 0s 432us/sample - loss: 0.6533 - acc: 0.7500\n",
            "Epoch 62/100\n",
            "112/112 [==============================] - 0s 397us/sample - loss: 0.7671 - acc: 0.7143\n",
            "Epoch 63/100\n",
            "112/112 [==============================] - 0s 397us/sample - loss: 0.7160 - acc: 0.7411\n",
            "Epoch 64/100\n",
            "112/112 [==============================] - 0s 355us/sample - loss: 0.5583 - acc: 0.8125\n",
            "Epoch 65/100\n",
            "112/112 [==============================] - 0s 354us/sample - loss: 0.7208 - acc: 0.6964\n",
            "Epoch 66/100\n",
            "112/112 [==============================] - 0s 349us/sample - loss: 0.6352 - acc: 0.7589\n",
            "Epoch 67/100\n",
            "112/112 [==============================] - 0s 390us/sample - loss: 0.6893 - acc: 0.8125\n",
            "Epoch 68/100\n",
            "112/112 [==============================] - 0s 623us/sample - loss: 0.7118 - acc: 0.7768\n",
            "Epoch 69/100\n",
            "112/112 [==============================] - 0s 455us/sample - loss: 0.7464 - acc: 0.7232\n",
            "Epoch 70/100\n",
            "112/112 [==============================] - 0s 517us/sample - loss: 0.6202 - acc: 0.7679\n",
            "Epoch 71/100\n",
            "112/112 [==============================] - 0s 389us/sample - loss: 0.6164 - acc: 0.7768\n",
            "Epoch 72/100\n",
            "112/112 [==============================] - 0s 421us/sample - loss: 0.6647 - acc: 0.7589\n",
            "Epoch 73/100\n",
            "112/112 [==============================] - 0s 409us/sample - loss: 0.5213 - acc: 0.7857\n",
            "Epoch 74/100\n",
            "112/112 [==============================] - 0s 335us/sample - loss: 0.5406 - acc: 0.8125\n",
            "Epoch 75/100\n",
            "112/112 [==============================] - 0s 391us/sample - loss: 0.6088 - acc: 0.8036\n",
            "Epoch 76/100\n",
            "112/112 [==============================] - 0s 443us/sample - loss: 0.6942 - acc: 0.7589\n",
            "Epoch 77/100\n",
            "112/112 [==============================] - 0s 353us/sample - loss: 0.6510 - acc: 0.7500\n",
            "Epoch 78/100\n",
            "112/112 [==============================] - 0s 407us/sample - loss: 0.6490 - acc: 0.7679\n",
            "Epoch 79/100\n",
            "112/112 [==============================] - 0s 394us/sample - loss: 0.5607 - acc: 0.7768\n",
            "Epoch 80/100\n",
            "112/112 [==============================] - 0s 463us/sample - loss: 0.5642 - acc: 0.8125\n",
            "Epoch 81/100\n",
            "112/112 [==============================] - 0s 384us/sample - loss: 0.5473 - acc: 0.7946\n",
            "Epoch 82/100\n",
            "112/112 [==============================] - 0s 423us/sample - loss: 0.5968 - acc: 0.7946\n",
            "Epoch 83/100\n",
            "112/112 [==============================] - 0s 395us/sample - loss: 0.6989 - acc: 0.7946\n",
            "Epoch 84/100\n",
            "112/112 [==============================] - 0s 397us/sample - loss: 0.8021 - acc: 0.6964\n",
            "Epoch 85/100\n",
            "112/112 [==============================] - 0s 444us/sample - loss: 0.5576 - acc: 0.8125\n",
            "Epoch 86/100\n",
            "112/112 [==============================] - 0s 402us/sample - loss: 0.5862 - acc: 0.7411\n",
            "Epoch 87/100\n",
            "112/112 [==============================] - 0s 401us/sample - loss: 0.5738 - acc: 0.8036\n",
            "Epoch 88/100\n",
            "112/112 [==============================] - 0s 363us/sample - loss: 0.8541 - acc: 0.6607\n",
            "Epoch 89/100\n",
            "112/112 [==============================] - 0s 344us/sample - loss: 0.5634 - acc: 0.7946\n",
            "Epoch 90/100\n",
            "112/112 [==============================] - 0s 432us/sample - loss: 0.5340 - acc: 0.7946\n",
            "Epoch 91/100\n",
            "112/112 [==============================] - 0s 397us/sample - loss: 0.5942 - acc: 0.7679\n",
            "Epoch 92/100\n",
            "112/112 [==============================] - 0s 390us/sample - loss: 0.5784 - acc: 0.8036\n",
            "Epoch 93/100\n",
            "112/112 [==============================] - 0s 447us/sample - loss: 0.4064 - acc: 0.8571\n",
            "Epoch 94/100\n",
            "112/112 [==============================] - 0s 373us/sample - loss: 0.6466 - acc: 0.7589\n",
            "Epoch 95/100\n",
            "112/112 [==============================] - 0s 427us/sample - loss: 0.5705 - acc: 0.7679\n",
            "Epoch 96/100\n",
            "112/112 [==============================] - 0s 505us/sample - loss: 0.5065 - acc: 0.8036\n",
            "Epoch 97/100\n",
            "112/112 [==============================] - 0s 399us/sample - loss: 0.6771 - acc: 0.7500\n",
            "Epoch 98/100\n",
            "112/112 [==============================] - 0s 344us/sample - loss: 0.6756 - acc: 0.7679\n",
            "Epoch 99/100\n",
            "112/112 [==============================] - 0s 397us/sample - loss: 0.4541 - acc: 0.8393\n",
            "Epoch 100/100\n",
            "112/112 [==============================] - 0s 398us/sample - loss: 0.7273 - acc: 0.7500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2954005065717195, 0.9736842]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\n",
        "# *********************************************************************************************************************\n",
        "  \n",
        "  # File Name \t:   CSLAB_DEEP_LEARNNG_RANDOM_FOREST_V1\n",
        "  # Purpose \t:   A Program in Python for Random Forest - Deep Learning\n",
        "  # Author\t:   Deepsphere.ai\n",
        "  # Reviewer \t:   Jothi Periasamy\n",
        "  # Date \t:   28/10/2022\n",
        "  # Version\t:   1.0\t\n",
        "  \n",
        "# ***********************************************************************************************************************\n",
        "\n",
        "## Program Description : Program for Random Forest - Deep Learning in Python\n",
        "\n",
        "## Python Development Environment & Runtime - Python, Anaconda, Tensorflow, Tensorboard\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as vAR_np\n",
        "\n",
        "import sklearn\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "import pandas as vAR_pd\n",
        "\n",
        "import tensorflow.compat.v1 as vAR_tf\n",
        "\n",
        "vAR_tf.disable_v2_behavior() \n",
        "\n",
        "#from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
        "\n",
        "from tensorflow.python.ops import resources\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "vAR_CSLAB_data = vAR_pd.read_csv('/content/drive/MyDrive/Semester-4/Deep Learning for Enterprise/Data/Iris.csv')\n",
        "\n",
        "vAR_CSLAB_input_x = vAR_CSLAB_data.iloc[:, 0:-1].values\n",
        "\n",
        "vAR_CSLAB_input_y = vAR_CSLAB_data.iloc[:,4].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "vAR_CSLAB_X_train, vAR_CSLAB_X_test, vAR_CSLAB_y_train, vAR_CSLAB_y_test = train_test_split(vAR_CSLAB_input_x, vAR_CSLAB_input_y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "vAR_CSLAB_data1 = vAR_CSLAB_data.iloc[:,:].values\n",
        "\n",
        "# Parameters\n",
        "\n",
        "vAR_CSLAB_num_steps = 100 # Total steps to train\n",
        "\n",
        "vAR_CSLAB_num_classes = 2 \n",
        "\n",
        "vAR_CSLAB_num_features = 108 \n",
        "\n",
        "vAR_CSLAB_num_trees = 10 \n",
        "\n",
        "vAR_CSLAB_max_nodes = 1000\n",
        "\n",
        "# Input and Target placeholders \n",
        "\n",
        "vAR_CSLAB_X = vAR_tf.placeholder(vAR_tf.float32, shape=[None, vAR_CSLAB_num_features])\n",
        "\n",
        "vAR_CSLAB_Y = vAR_tf.placeholder(vAR_tf.int64, shape=[None])\n",
        "\n",
        "vAR_CSLAB_model = vAR_tf.keras.models.Sequential()\n",
        "\n",
        "vAR_CSLAB_model.add(vAR_tf.keras.layers.Dense(10, input_dim=4, activation=vAR_tf.nn.relu, kernel_initializer='he_normal', \n",
        "                                kernel_regularizer=vAR_tf.keras.regularizers.l2(0.01)))\n",
        "\n",
        "vAR_CSLAB_model.add(vAR_tf.keras.layers.BatchNormalization())\n",
        "vAR_CSLAB_model.add(vAR_tf.keras.layers.Dropout(0.3))\n",
        "vAR_CSLAB_model.add(vAR_tf.keras.layers.Dense(7, activation=vAR_tf.nn.relu, kernel_initializer='he_normal', \n",
        "                                kernel_regularizer=vAR_tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
        "vAR_CSLAB_model.add(vAR_tf.keras.layers.BatchNormalization())\n",
        "vAR_CSLAB_model.add(vAR_tf.keras.layers.Dropout(0.3))\n",
        "vAR_CSLAB_model.add(vAR_tf.keras.layers.Dense(5, activation=vAR_tf.nn.relu, kernel_initializer='he_normal', \n",
        "                                kernel_regularizer=vAR_tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
        "vAR_CSLAB_model.add(vAR_tf.keras.layers.Dense(3, activation=vAR_tf.nn.softmax))\n",
        "\n",
        "vAR_CSLAB_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#vAR_CSLAB_y_train\n",
        "\n",
        "vAR_CSLAB_iris_model = vAR_CSLAB_model.fit(vAR_CSLAB_X_train, vAR_CSLAB_y_train, epochs=100, batch_size=7)\n",
        "\n",
        "vAR_CSLAB_model.evaluate(vAR_CSLAB_X_test, vAR_CSLAB_y_test)\n",
        "\n",
        "# ****************************************************************************************************************************\n",
        "#   Disclaimer.\n",
        "\n",
        "# We are providing this code block strictly for learning and researching, this is not a production\n",
        "# ready code. We have no liability on this particular code under any circumstances; users should use\n",
        "# this code on their own risk. All software, hardware and othr products that are referenced in these \n",
        "# materials belong to the respective vendor who developed or who owns this product.\n",
        "\n",
        "# ****************************************************************************************************************************\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Explanation**\n",
        "\n",
        "This is a script written in Python that implements a multi-layer neural network for classification using TensorFlow.\n",
        "\n",
        "The first part of the script imports several libraries and modules. \"future\" is a module in Python that allows programmers to use future language features. In this case, \"print_function\" is imported so that the print statement can be used as a function."
      ],
      "metadata": {
        "id": "u7ezqzj-Amr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"numpy\" and \"pandas\" are imported as vAR_np and vAR_pd, respectively, and are used for numerical and data manipulation. \"sklearn\" is a machine learning library, and \"datasets\" is a module from sklearn that contains built-in datasets for testing and demonstration purposes.\n",
        "\n",
        "\"tensorflow.compat.v1 as vAR_tf\" imports TensorFlow version 1.x, which is the version used in this script, and renames it as vAR_tf. The line \"vAR_tf.disable_v2_behavior()\" disables the behavior of TensorFlow 2.x.\n"
      ],
      "metadata": {
        "id": "EUIrlsuzAmhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script also imports \"resources\" from \"tensorflow.python.ops\" and \"drive\" from \"google.colab\". The \"drive.mount('/content/drive')\" line is used to mount Google Drive to the Colab environment.\n",
        "\n",
        "The Iris dataset is read from a CSV file located in Google Drive and stored in a pandas dataframe named \"vAR_CSLAB_data\". The inputs and targets are separated into \"vAR_CSLAB_input_x\" and \"vAR_CSLAB_input_y\", respectively, by selecting the columns from the dataframe."
      ],
      "metadata": {
        "id": "mAZuvPUWAmVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inputs are then split into training and testing sets using the \"train_test_split\" function from the sklearn library.\n",
        "\n",
        "Several parameters for the model are defined, including the number of steps to train, the number of classes, the number of features, the number of trees, and the maximum number of nodes.\n",
        "\n",
        "The inputs and targets are defined as placeholders in TensorFlow using the \"vAR_tf.placeholder\" function, with their shapes specified."
      ],
      "metadata": {
        "id": "bzZ6Mv4LAmJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sequential model is created using the \"vAR_tf.keras.models.Sequential\" method and various dense layers are added to the model using the \"add\" method. Each layer specifies the number of neurons, activation function, and regularization methods used. The final layer uses a softmax activation function, which outputs probabilities that sum to 1, suitable for multi-class classification.\n",
        "\n",
        "The model is compiled using the \"compile\" method, specifying the optimizer, loss function, and evaluation metric. In this case, the optimizer is \"adam\", the loss function is \"sparse_categorical_crossentropy\", and the metric is accuracy.\n",
        "\n",
        "The model is then fit to the training data using the \"fit\" method and the accuracy of the model is evaluated on the testing data using the \"evaluate\" method."
      ],
      "metadata": {
        "id": "8g1ZU1CIAl9d"
      }
    }
  ]
}