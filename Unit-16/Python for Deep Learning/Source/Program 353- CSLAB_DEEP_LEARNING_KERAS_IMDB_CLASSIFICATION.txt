
/*********************************************************************************************************************
  
  File Name 	:   CSLAB_DEEP_LEARNING_KERAS_IMDB_CLASSIFICATION
  Purpose 	:   A Program in Python for IMDB Movie Classification using Keras Library in Deep Learning
  Author	:   Durga Prasad
  Reviewer 	:   Jothi Periasamy
  Date and Time	:   07/01/2019 16:49 hrs
  Version	:   1.0	
  Change History: 
_______________________________________________________________________________________________________________________

   Who				   When 			        Why
_______________________________________________________________________________________________________________________


    DP				07/01/2019		           Initital Release 

________________________________________________________________________________________________________________________


/***********************************************************************************************************************

## Program Description : Program in Python for IMDB Movie Classification using Keras Library in Deep Learning

## Python Development Environment & Runtime - Python, Anaconda

from __future__ import print_function

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D
from keras.datasets import imdb

# set parameters:
vAR_CSLAB_max_features = 5000
vAR_CSLAB_maxlen = 400
vAR_CSLAB_batch_size = 32
vAR_CSLAB_embedding_dims = 50
vAR_CSLAB_filters = 250
vAR_CSLAB_kernel_size = 3
vAR_CSLAB_hidden_dims = 250
vAR_CSLAB_epochs = 1

print('Loading data...')
(vAR_CSLAB_x_train, vAR_CSLAB_y_train), (vAR_CSLAB_x_test, vAR_CSLAB_y_test) = imdb.load_data(num_words=vAR_CSLAB_max_features)
print(len(vAR_CSLAB_x_train), 'train sequences')
print(len(vAR_CSLAB_x_test), 'test sequences')

print('Pad sequences (samples x time)')
vAR_CSLAB_x_train = sequence.pad_sequences(vAR_CSLAB_x_train, maxlen=vAR_CSLAB_maxlen)
vAR_CSLAB_x_test = sequence.pad_sequences(vAR_CSLAB_x_test, maxlen=vAR_CSLAB_maxlen)
print('x_train shape:', vAR_CSLAB_x_train.shape)
print('x_test shape:', vAR_CSLAB_x_test.shape)

print('Build model...')
vAR_CSLAB_model = Sequential()

# we start off with an efficient embedding layer which maps
# our vocab indices into embedding_dims dimensions
vAR_CSLAB_model.add(Embedding(vAR_CSLAB_max_features,
                    vAR_CSLAB_embedding_dims,
                    input_length=vAR_CSLAB_maxlen))
vAR_CSLAB_model.add(Dropout(0.2))

# we add a Convolution1D, which will learn filters
# word group filters of size filter_length:
vAR_CSLAB_model.add(Conv1D(vAR_CSLAB_filters,
                 vAR_CSLAB_kernel_size,
                 padding='valid',
                 activation='relu',
                 strides=1))
# we use max pooling:
vAR_CSLAB_model.add(GlobalMaxPooling1D())

# We add a vanilla hidden layer:
vAR_CSLAB_model.add(Dense(vAR_CSLAB_hidden_dims))
vAR_CSLAB_model.add(Dropout(0.2))
vAR_CSLAB_model.add(Activation('relu'))

# We project onto a single unit output layer, and squash it with a sigmoid:
vAR_CSLAB_model.add(Dense(1))
vAR_CSLAB_model.add(Activation('sigmoid'))

vAR_CSLAB_model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
vAR_CSLAB_model.fit(vAR_CSLAB_x_train, vAR_CSLAB_y_train,
          batch_size=vAR_CSLAB_batch_size,
          epochs=vAR_CSLAB_epochs,
          validation_data=(vAR_CSLAB_x_test, vAR_CSLAB_y_test))


/****************************************************************************************************************************
  Disclaimer.

We are providing this code block strictly for learning and researching, this is not a production
ready code. We have no liability on this particular code under any circumstances; users should use
this code on their own risk. All software, hardware and othr products that are referenced in these 
materials belong to the respective vendor who developed or who owns this product.

/****************************************************************************************************************************
  