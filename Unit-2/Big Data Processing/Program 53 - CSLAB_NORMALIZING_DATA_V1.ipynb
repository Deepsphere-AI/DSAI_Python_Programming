{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepsphere-AI/DSAI_Python_Programming/blob/main/Unit-2/Big%20Data%20Processing/Program%2053%20-%20CSLAB_NORMALIZING_DATA_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instruction to Follow:\n",
        "\n",
        "The Code given Below works on the google cloud environment with the Big Data environment (DataProc) and Hadoop file storage system Setup. It cannot be executed here in the google colaboratory notebook.  "
      ],
      "metadata": {
        "id": "rfAIVZTn0t5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# *********************************************************************************************************************\n",
        "  \n",
        "  # File Name \t:   CSLAB_NORMALIZING_DATA_V1\n",
        "  # Purpose \t:   A Program in Python for Normalizing Data for Big Data Processing\n",
        "  # Author\t:   Deepsphere.ai\n",
        "  # Reviewer \t:   Jothi Periasamy\n",
        "  # Date \t:   10/25/2022 \n",
        "  # Version\t:   1.0\t\n",
        "  \n",
        "# ***********************************************************************************************************************\n",
        "\n",
        "## Program Description : Program for Normalizing Data for Big Data Processing\n",
        "\n",
        "## Python Development Environment & Runtime - Python, PySpark,Anaconda\n",
        "\n",
        "import pyspark \n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"hdfs_test\").getOrCreate()\n",
        "\n",
        "vAR_df = spark.read.format('csv').load(\"gs://patientreadmission/Patient Readmission.csv\",header=True)\n",
        "\n",
        "vAR_df1 = vAR_df.toPandas()\n",
        "\n",
        "vAR_df1.head()\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "vAR_df1 = vAR_df1[['patient_nbr', 'Weight(KG)','Education Level (Grade)','admission_type_id', 'discharge_disposition_id','admission_source_id','time_in_hospital','num_lab_procedures','num_procedures','num_medications']]\n",
        "\n",
        "vAR_normalized_Features = preprocessing.normalize(vAR_df1)\n",
        "\n",
        "vAR_normalized_Features_df = pd.DataFrame(vAR_normalized_Features)\n",
        "\n",
        "vAR_normalized_Features_df.head()\n",
        "\n",
        "# ****************************************************************************************************************************\n",
        "#   Disclaimer.\n",
        "\n",
        "# We are providing this code block strictly for learning and researching, this is not a production\n",
        "# ready code. We have no liability on this particular code under any circumstances; users should use\n",
        "# this code on their own risk. All software, hardware and othr products that are referenced in these \n",
        "# materials belong to the respective vendor who developed or who owns this product.\n",
        "\n",
        "# ****************************************************************************************************************************\n",
        "  "
      ],
      "metadata": {
        "id": "JJ0tuTGs0qpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}