{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepsphere-AI/DSAI_Python_Programming/blob/main/Unit-2/Big%20Data%20Processing/Program%2051%20-%20CSLAB_READING_WRITING_DATA_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instruction to Follow:\n",
        "\n",
        "The Code given Below works on the google cloud environment with the Big Data environment (DataProc) and Hadoop file storage system Setup. It cannot be executed here in the google colaboratory notebook.  "
      ],
      "metadata": {
        "id": "NOFRj_Q5dEIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# *********************************************************************************************************************\n",
        "  \n",
        "  # File Name \t:   CSLAB_READING_WRITING_DATA_V1\n",
        "  # Purpose \t:   A Program for Reading & Writing Data\n",
        "  # Author\t:   Deepshere.ai\n",
        "  # Reviewer \t:   Jothi Periasamy\n",
        "  # Date \t:   10/25/2022 \n",
        "  # Version\t:   1.0\t\n",
        "  \n",
        "# ***********************************************************************************************************************\n",
        "\n",
        "## Program Description : Program for Reading & Writing Data for Big Data Processing\n",
        "\n",
        "## Python Development Environment & Runtime - Python, Pyspark, Anaconda\n",
        "\n",
        "import pyspark \n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"hdfs_test\").getOrCreate()\n",
        "\n",
        "import pandas as vAR_pd\n",
        "\n",
        "vAR_df = spark.read.format('text').load(\"hdfs:///user/dataproc/Patient Readmission.txt\",header=True)\n",
        "\n",
        "vAR_df.head()\n",
        "\n",
        "##*****************************************************************\n",
        "\n",
        "## Reading the data from the Google Storage Bucket\n",
        "\n",
        "#vAR_df = spark.read.csv(\"gs://patientreadmission/Patient Readmission.csv\",header=True,inferSchema=True)\n",
        "\n",
        "vAR_df = spark.read.format('text').load(\"gs://patientreadmission/Patient Readmission.txt\")\n",
        "\n",
        "vAR_df.head()\n",
        "\n",
        "vAR_df1 = vAR_df.toPandas()\n",
        "\n",
        "## Writing the data  back to  the Google Cloud Storage Bucket\n",
        "\n",
        "vAR_df1.to_csv(\"gs://patientreadmission/Patient Readmission1.txt\")\n",
        "\n",
        "##*****************************************************************\n",
        "\n",
        "# ****************************************************************************************************************************\n",
        "#   Disclaimer.\n",
        "\n",
        "# We are providing this code block strictly for learning and researching, this is not a production\n",
        "# ready code. We have no liability on this particular code under any circumstances; users should use\n",
        "# this code on their own risk. All software, hardware and othr products that are referenced in these \n",
        "# materials belong to the respective vendor who developed or who owns this product.\n",
        "\n",
        "# ****************************************************************************************************************************\n",
        "  \n"
      ],
      "metadata": {
        "id": "0MqbGNVEc-sD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}